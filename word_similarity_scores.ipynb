{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import brown, wordnet as wn, stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse.linalg import svds\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the brown dataset (which contains around 1M tokens) and the wordnet knowledge base to create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/encrypted_soul/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/encrypted_soul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download datasets\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading human annotated datasets which will be later used for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_path = 'human_annoted_scores/SimLex-999/SimLex-999.txt'\n",
    "simlex_df = pd.read_csv(simlex_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'human_annoted_scores/wordsim353crowd.csv'\n",
    "wordsim_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>A</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>A</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>A</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>A</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
       "0    old          new   A       1.58      2.72      2.81      2        7.25   \n",
       "1  smart  intelligent   A       9.20      1.75      2.46      1        7.11   \n",
       "2   hard    difficult   A       8.77      3.76      2.21      2        5.94   \n",
       "3  happy     cheerful   A       9.55      2.56      2.34      1        5.85   \n",
       "4   hard         easy   A       0.95      3.76      2.07      2        5.82   \n",
       "\n",
       "   SimAssoc333  SD(SimLex)  \n",
       "0            1        0.41  \n",
       "1            1        0.67  \n",
       "2            1        1.19  \n",
       "3            1        2.18  \n",
       "4            1        0.93  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Human (Mean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>admission</td>\n",
       "      <td>ticket</td>\n",
       "      <td>5.5360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>chemistry</td>\n",
       "      <td>4.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aluminum</td>\n",
       "      <td>metal</td>\n",
       "      <td>6.6250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>announcement</td>\n",
       "      <td>effort</td>\n",
       "      <td>2.0625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>announcement</td>\n",
       "      <td>news</td>\n",
       "      <td>7.1875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word 1     Word 2  Human (Mean)\n",
       "0     admission     ticket        5.5360\n",
       "1       alcohol  chemistry        4.1250\n",
       "2      aluminum      metal        6.6250\n",
       "3  announcement     effort        2.0625\n",
       "4  announcement       news        7.1875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions used for enriching with wordnet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synset_depth(word):\n",
    "    depths = [synset.min_depth() for synset in wn.synsets(word)]\n",
    "    return np.mean(depths) if depths else 0\n",
    "\n",
    "def hypernym_count(word):\n",
    "    return sum(len(synset.hypernyms()) for synset in wn.synsets(word))\n",
    "\n",
    "def hyponym_count(word):\n",
    "    return sum(len(synset.hyponyms()) for synset in wn.synsets(word))\n",
    "\n",
    "def meronym_holonym_count(word):\n",
    "    meronyms = sum(len(synset.part_meronyms()) + len(synset.substance_meronyms()) for synset in wn.synsets(word))\n",
    "    holonyms = sum(len(synset.part_holonyms()) + len(synset.substance_holonyms()) for synset in wn.synsets(word))\n",
    "    return meronyms, holonyms\n",
    "\n",
    "def has_antonym(word):\n",
    "    for synset in wn.synsets(word):\n",
    "        for lemma in synset.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to enrich embeddings with wordnet features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_wordnet_features(word_vecs):\n",
    "    enriched_vecs = defaultdict(list)\n",
    "    for word, vec in word_vecs.items():\n",
    "        features = [\n",
    "            len(wn.synsets(word)),\n",
    "            synset_depth(word),\n",
    "            hypernym_count(word),\n",
    "            hyponym_count(word),\n",
    "            *meronym_holonym_count(word),\n",
    "            has_antonym(word)\n",
    "        ]\n",
    "        enriched_vecs[word] = np.concatenate([vec, features])\n",
    "    return enriched_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Brown Corpus contains 1161192 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Load the Brown Corpus and count the tokens to verify that the corpus has around 1M tokens\n",
    "brown_tokens = brown.words()\n",
    "num_tokens = len(brown_tokens)\n",
    "\n",
    "print(f\"The Brown Corpus contains {num_tokens} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the structure of sentences\n",
    "brown_corpus = brown.sents()\n",
    "brown_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Daniel personally led the fight for the measure , which he had watered down considerably since its rejection by two previous Legislatures , in a public hearing before the House Committee on Revenue and Taxation .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\" \".join(sent) for sent in brown_corpus]\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", token_pattern=r\"\\b\\w+\\b\", min_df=1)\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "sentences[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the co-occurrence matrix (A, B) where A is the number of unique sentences and B is the number of unique tokens: (57340, 42432)\n"
     ]
    }
   ],
   "source": [
    "dense_X = X.todense()\n",
    "\n",
    "print(\"Shape of the co-occurrence matrix (A, B) where A is the number of unique sentences and B is the number of unique tokens:\", dense_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_float = csr_matrix(X, dtype=np.float64)\n",
    "# reducing the dimension of the matrix\n",
    "u, s, vt = svds(X_float, k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map words to their vector representations\n",
    "word_vecs = {word: vt[:, i] for i, word in enumerate(vectorizer.get_feature_names_out())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.28757506e-04 -3.54495918e-03 -3.84851971e-04  4.33727754e-03\n",
      " -1.24188563e-03 -1.92492773e-03  1.59946444e-03  1.87134374e-03\n",
      " -2.51345921e-04 -8.14635967e-04  8.63220753e-04 -1.67919458e-04\n",
      "  1.35632261e-03  4.30029795e-05 -1.61946393e-03 -2.20584179e-03\n",
      " -1.26405196e-03 -5.56515921e-04  2.67273927e-03  9.67846535e-04\n",
      " -9.14451485e-04  1.40411393e-03  1.05480906e-03  1.06223881e-03\n",
      " -4.38454834e-04  2.59497214e-04 -1.17849114e-03 -5.15815387e-04\n",
      "  1.38160853e-03  9.01050822e-04  2.31521988e-04  7.25737582e-04\n",
      " -4.92206423e-04  1.06031907e-04 -1.82074803e-03  1.51335466e-03\n",
      "  4.64471263e-04 -3.55045689e-04  7.91528813e-04 -5.52454731e-04\n",
      " -1.37833998e-03 -2.05704898e-03  5.74992479e-04 -3.74322914e-04\n",
      " -3.57905685e-04 -9.07370164e-05 -5.88274640e-04  3.05916056e-04\n",
      "  8.99795412e-04 -2.44268757e-04 -7.85604194e-04  1.55800712e-03\n",
      "  3.28604257e-04  1.07089846e-05  7.46523256e-04 -1.60337490e-03\n",
      "  3.81408137e-04 -2.23378820e-04 -1.78856156e-04  5.76030321e-04\n",
      "  3.28439876e-04 -1.59798966e-05  2.58757248e-04 -1.61025633e-03\n",
      " -2.72522333e-04 -2.83731126e-04  1.75780244e-04  5.99594148e-04\n",
      "  6.31072995e-04 -1.54317737e-03  1.11548176e-04  2.37071871e-04\n",
      " -9.93691189e-05  8.02108639e-04 -1.59756880e-03 -8.57974198e-04\n",
      "  3.98916401e-04 -4.23107580e-04  8.79673914e-04  7.02952024e-04\n",
      "  6.77272613e-04  1.00277705e-03  4.95899864e-04 -6.72616528e-04\n",
      "  6.21579121e-05 -4.87097037e-04  1.26395227e-04 -1.41917393e-04\n",
      "  8.94177174e-05 -7.23446713e-04  1.51640958e-04 -1.38051580e-04\n",
      " -3.34564583e-04 -7.25591873e-04 -4.09046261e-04  1.52444492e-03\n",
      " -7.40369012e-04  4.21752030e-04  9.65334200e-04  5.07202599e-04\n",
      "  8.00000000e+00  6.62500000e+00  9.00000000e+00  2.40000000e+01\n",
      "  1.00000000e+00  2.00000000e+00  0.00000000e+00]\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "enriched_word_vecs = enrich_wordnet_features(word_vecs)\n",
    "\n",
    "# Example: Access enriched vector for 'dog'\n",
    "print(enriched_word_vecs['dog'])\n",
    "print(len(enriched_word_vecs['dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_similarities = []\n",
    "for _, row in simlex_df.iterrows():\n",
    "    word1, word2 = row['word1'], row['word2']\n",
    "    if word1 in enriched_word_vecs and word2 in enriched_word_vecs:\n",
    "        vec1 = enriched_word_vecs[word1]\n",
    "        vec2 = enriched_word_vecs[word2]\n",
    "        similarity = get_cosine_similarity(vec1, vec2)\n",
    "        computed_similarities.append(similarity)\n",
    "    else:\n",
    "        computed_similarities.append(np.nan)  # If one of the words is not in the embeddings, treat as missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "simlex_df['computed_similarity'] = computed_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>POS</th>\n",
       "      <th>SimLex999</th>\n",
       "      <th>conc(w1)</th>\n",
       "      <th>conc(w2)</th>\n",
       "      <th>concQ</th>\n",
       "      <th>Assoc(USF)</th>\n",
       "      <th>SimAssoc333</th>\n",
       "      <th>SD(SimLex)</th>\n",
       "      <th>computed_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>old</td>\n",
       "      <td>new</td>\n",
       "      <td>A</td>\n",
       "      <td>1.58</td>\n",
       "      <td>2.72</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>7.25</td>\n",
       "      <td>1</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.988567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smart</td>\n",
       "      <td>intelligent</td>\n",
       "      <td>A</td>\n",
       "      <td>9.20</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.46</td>\n",
       "      <td>1</td>\n",
       "      <td>7.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.730278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hard</td>\n",
       "      <td>difficult</td>\n",
       "      <td>A</td>\n",
       "      <td>8.77</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.21</td>\n",
       "      <td>2</td>\n",
       "      <td>5.94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.913758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy</td>\n",
       "      <td>cheerful</td>\n",
       "      <td>A</td>\n",
       "      <td>9.55</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.34</td>\n",
       "      <td>1</td>\n",
       "      <td>5.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.976183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hard</td>\n",
       "      <td>easy</td>\n",
       "      <td>A</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.76</td>\n",
       "      <td>2.07</td>\n",
       "      <td>2</td>\n",
       "      <td>5.82</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.999776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word1        word2 POS  SimLex999  conc(w1)  conc(w2)  concQ  Assoc(USF)  \\\n",
       "0    old          new   A       1.58      2.72      2.81      2        7.25   \n",
       "1  smart  intelligent   A       9.20      1.75      2.46      1        7.11   \n",
       "2   hard    difficult   A       8.77      3.76      2.21      2        5.94   \n",
       "3  happy     cheerful   A       9.55      2.56      2.34      1        5.85   \n",
       "4   hard         easy   A       0.95      3.76      2.07      2        5.82   \n",
       "\n",
       "   SimAssoc333  SD(SimLex)  computed_similarity  \n",
       "0            1        0.41             0.988567  \n",
       "1            1        0.67             0.730278  \n",
       "2            1        1.19             0.913758  \n",
       "3            1        2.18             0.976183  \n",
       "4            1        0.93             0.999776  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation between computed similarities and human judgments: 0.060776462864572396\n"
     ]
    }
   ],
   "source": [
    "filtered_simlex_df = simlex_df.dropna(subset=['computed_similarity'])\n",
    "spearman_corr, _ = spearmanr(filtered_simlex_df['SimLex999'], filtered_simlex_df['computed_similarity'])\n",
    "print(f\"Spearman Correlation between computed similarities and human judgments: {spearman_corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we got an extremely bad score with the embeddings we just created with some simple feature engineering of wordnet knowledge base. Lets build another embedding using a simple correlation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation coefficient: -0.06221871077974628\n"
     ]
    }
   ],
   "source": [
    "corpus = brown.words()\n",
    "corpus = [word.lower() for word in corpus]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "corpus = [word for word in corpus if word not in stop_words and word not in string.punctuation]\n",
    "vocabulary = sorted(set(corpus))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Create a co-occurence matrix\n",
    "window_size = 3\n",
    "co_occurrence_matrix = np.zeros((len(vocabulary), len(vocabulary)))\n",
    "\n",
    "for i in range(len(corpus) - window_size + 1):\n",
    "    window = corpus[i:i+window_size]\n",
    "    for j in range(window_size):\n",
    "        for k in range(window_size):\n",
    "            if j != k:\n",
    "                word1 = window[j]\n",
    "                word2 = window[k]\n",
    "                idx1 = word2idx[word1]\n",
    "                idx2 = word2idx[word2]\n",
    "                co_occurrence_matrix[idx1, idx2] += 1\n",
    "\n",
    "# Calculate similarity scores for word pairs in SimLex-999\n",
    "computed_similarities = []\n",
    "for _, row in simlex_df.iterrows():\n",
    "    word1, word2 = row['word1'], row['word2']\n",
    "    if word1 in word2idx and word2 in word2idx:\n",
    "        idx1 = word2idx[word1]\n",
    "        idx2 = word2idx[word2]\n",
    "        vec1 = co_occurrence_matrix[idx1]\n",
    "        vec2 = co_occurrence_matrix[idx2]\n",
    "        similarity = get_cosine_similarity(vec1, vec2)\n",
    "        computed_similarities.append(similarity)\n",
    "    else:\n",
    "        computed_similarities.append(np.nan)\n",
    "\n",
    "simlex_df['computed_similarity'] = computed_similarities\n",
    "filtered_simlex_df = simlex_df.dropna(subset=['computed_similarity'])\n",
    "spearman_corr, _ = spearmanr(filtered_simlex_df['SimLex999'], filtered_simlex_df['computed_similarity'])\n",
    "print(f\"Spearman's rank correlation coefficient: {spearman_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, sentences, vector_size, window_size, learning_rate, epochs):\n",
    "        self.sentences = sentences\n",
    "        self.vector_size = vector_size\n",
    "        self.window_size = window_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.word_counts = defaultdict(int)\n",
    "        self.word_to_index = {}\n",
    "        self.index_to_word = {}\n",
    "        self.word_vectors = None\n",
    "\n",
    "    def preprocess(self):\n",
    "        vocab = set()\n",
    "        for sentence in self.sentences:\n",
    "            for word in sentence:\n",
    "                self.word_counts[word] += 1\n",
    "                vocab.add(word)\n",
    "\n",
    "        self.word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "        self.index_to_word = {index: word for word, index in self.word_to_index.items()}\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.word_vectors = np.random.uniform(-0.5, 0.5, (self.vocab_size, self.vector_size))\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for sentence in self.sentences:\n",
    "                for center_word_pos, center_word in enumerate(sentence):\n",
    "                    center_word_index = self.word_to_index[center_word]\n",
    "                    context_words = self.get_context_words(sentence, center_word_pos)\n",
    "\n",
    "                    for context_word in context_words:\n",
    "                        context_word_index = self.word_to_index[context_word]\n",
    "                        self.update_weights(center_word_index, context_word_index)\n",
    "\n",
    "    def train_with_wordnet(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for sentence in self.sentences:\n",
    "                for center_word_pos, center_word in enumerate(sentence):\n",
    "                    center_word_index = self.word_to_index[center_word]\n",
    "                    context_words = self.get_context_words(sentence, center_word_pos)\n",
    "\n",
    "                    for context_word in context_words:\n",
    "                        context_word_index = self.word_to_index[context_word]\n",
    "                        self.update_weights(center_word_index, context_word_index)\n",
    "                    synsets = wn.synsets(center_word)\n",
    "                    for synset in synsets:\n",
    "                        for lemma in synset.lemmas():\n",
    "                            if lemma.name() in self.word_to_index:\n",
    "                                related_word_index = self.word_to_index[lemma.name()]\n",
    "                                self.update_weights(center_word_index, related_word_index)\n",
    "                                self.update_weights(related_word_index, center_word_index)\n",
    "\n",
    "\n",
    "    def get_context_words(self, sentence, center_word_pos):\n",
    "        start = max(0, center_word_pos - self.window_size)\n",
    "        end = min(len(sentence), center_word_pos + self.window_size + 1)\n",
    "        return [word for word in sentence[start:end] if word != sentence[center_word_pos]]\n",
    "\n",
    "    def update_weights(self, center_word_index, context_word_index):\n",
    "        center_vector = self.word_vectors[center_word_index]\n",
    "        context_vector = self.word_vectors[context_word_index]\n",
    "\n",
    "        error = 1 - np.dot(center_vector, context_vector)\n",
    "        gradient = -error * context_vector\n",
    "        context_gradient = -error * center_vector\n",
    "\n",
    "        self.word_vectors[center_word_index] -= self.learning_rate * gradient\n",
    "        self.word_vectors[context_word_index] -= self.learning_rate * context_gradient\n",
    "\n",
    "    def most_similar(self, word, top_n=5):\n",
    "        word_index = self.word_to_index[word]\n",
    "        word_vector = self.word_vectors[word_index]\n",
    "\n",
    "        similarities = np.dot(self.word_vectors, word_vector)\n",
    "        top_indices = np.argsort(similarities)[-top_n-1:-1][::-1]\n",
    "\n",
    "        similar_words = [(self.index_to_word[index], similarities[index]) for index in top_indices]\n",
    "        return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman's rank correlation coefficient: -0.0869\n"
     ]
    }
   ],
   "source": [
    "sentences = [list(sent) for sent in brown.sents()]\n",
    "vector_size = 100\n",
    "window_size = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "model = Word2Vec(sentences, vector_size, window_size, learning_rate, epochs)\n",
    "model.preprocess()\n",
    "model.train_with_wordnet()\n",
    "\n",
    "simlex_path = 'human_annoted_scores/SimLex-999/SimLex-999.txt'\n",
    "simlex_df = pd.read_csv(simlex_path, sep='\\t')\n",
    "\n",
    "computed_similarities = []\n",
    "for _, row in simlex_df.iterrows():\n",
    "    word1, word2 = row['word1'], row['word2']\n",
    "    if word1 in model.word_to_index and word2 in model.word_to_index:\n",
    "        idx1 = model.word_to_index[word1]\n",
    "        idx2 = model.word_to_index[word2]\n",
    "        vec1 = model.word_vectors[idx1]\n",
    "        vec2 = model.word_vectors[idx2]\n",
    "        similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "        computed_similarities.append(similarity)\n",
    "    else:\n",
    "        computed_similarities.append(np.nan)\n",
    "\n",
    "simlex_df['computed_similarity'] = computed_similarities\n",
    "\n",
    "filtered_simlex_data = simlex_df.dropna(subset=['computed_similarity'])\n",
    "spearman_corr, _ = spearmanr(filtered_simlex_data['SimLex999'], filtered_simlex_data['computed_similarity'])\n",
    "print(f\"Spearman's rank correlation coefficient: {spearman_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Similarity Scores: Unconstrained\n",
    "\n",
    "We will be using the spearman's coefficient to evaluate the model with Wordsim-354 and SimLex-999 human annotated datasets. We will be using the glove model with 6B tokens and 50, 100, 200 and 300 vector dimension sizes for numeric representation of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file into a dictionary.\n",
    "\n",
    "    This function reads a GloVe embeddings file where each line corresponds to a word\n",
    "    and its vector representation. The function parses each line to extract the word and its\n",
    "    corresponding vector, and then stores these in a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The file path to the GloVe embeddings file. The file is expected to be in the\n",
    "                  format where each line contains a word followed by its embedding vector,\n",
    "                  with spaces as separators.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary where keys are words (str) and values are their corresponding vector\n",
    "            embeddings (numpy arrays of dtype 'float32').\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        embeddings = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spearman_correlation_with_simlex999(simlex_df, glove_embeddings):\n",
    "    \"\"\"\n",
    "    Calculate Spearman correlation between human-labeled similarity scores and computed cosine similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "    - simlex_df: DataFrame with columns 'word1', 'word2', and 'SimLex999' containing similarity scores.\n",
    "    - glove_embeddings: Dictionary mapping words to their vector embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - spearman_corr: Spearman correlation coefficient.\n",
    "    \"\"\"\n",
    "    computed_similarities = []\n",
    "    for _, row in simlex_df.iterrows():\n",
    "        word1, word2 = row['word1'], row['word2'].lower()\n",
    "        if word1 in glove_embeddings and word2 in glove_embeddings:\n",
    "            vec1 = glove_embeddings[word1]\n",
    "            vec2 = glove_embeddings[word2]\n",
    "            similarity = get_cosine_similarity(vec1, vec2)  # Using the same cosine_similarity function as before\n",
    "            computed_similarities.append(similarity)\n",
    "        else:\n",
    "            computed_similarities.append(np.nan)\n",
    "\n",
    "    simlex_df['computed_similarity'] = computed_similarities\n",
    "    filtered_simlex_df = simlex_df.dropna(subset=['computed_similarity'])\n",
    "    spearman_corr, _ = spearmanr(filtered_simlex_df['SimLex999'], filtered_simlex_df['computed_similarity'])\n",
    "    return spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_spearman_correlation_with_wordsim353(wordsim_df, glove_embeddings):\n",
    "    \"\"\"\n",
    "    Calculate Spearman correlation between human-labeled similarity scores and computed cosine similarity scores.\n",
    "\n",
    "    Parameters:\n",
    "    - wordsim_df: DataFrame with columns 'Word 1', 'Word 2', and 'Human (Mean)' containing similarity scores.\n",
    "    - glove_embeddings: Dictionary mapping words to their vector embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - spearman_corr: Spearman correlation coefficient.\n",
    "    \"\"\"\n",
    "    computed_similarities = []\n",
    "    for _, row in wordsim_df.iterrows():\n",
    "        word1, word2 = row['Word 1'].lower(), row['Word 2'].lower()\n",
    "        if word1 in glove_embeddings and word2 in glove_embeddings:\n",
    "            vec1 = glove_embeddings[word1]\n",
    "            vec2 = glove_embeddings[word2]\n",
    "            similarity = get_cosine_similarity(vec1, vec2)\n",
    "            computed_similarities.append(similarity)\n",
    "        else:\n",
    "            computed_similarities.append(np.nan)\n",
    "\n",
    "    wordsim_df['computed_similarity'] = computed_similarities\n",
    "    filtered_wordsim_df = wordsim_df.dropna(subset=['computed_similarity'])\n",
    "    spearman_corr, _ = spearmanr(filtered_wordsim_df['Human (Mean)'], filtered_wordsim_df['computed_similarity'])\n",
    "    return spearman_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman Correlation Value with SimLex-999 and glove.6B/glove.6B.50d.txt: 0.2645792192990813\n",
      "The Spearman Correlation Value with SimLex-999 and glove.6B/glove.6B.100d.txt: 0.29755501657418554\n",
      "The Spearman Correlation Value with SimLex-999 and glove.6B/glove.6B.200d.txt: 0.3402590362173856\n",
      "The Spearman Correlation Value with SimLex-999 and glove.6B/glove.6B.300d.txt: 0.37050035710869067\n"
     ]
    }
   ],
   "source": [
    "glove_paths = [\n",
    "    'glove.6B/glove.6B.50d.txt',\n",
    "    'glove.6B/glove.6B.100d.txt',\n",
    "    'glove.6B/glove.6B.200d.txt',\n",
    "    'glove.6B/glove.6B.300d.txt'\n",
    "]\n",
    "\n",
    "for path in glove_paths:\n",
    "    glove_embeddings = load_glove_embeddings(path=path)\n",
    "    print(f'The Spearman Correlation Value with SimLex-999 and {path}: {calculate_spearman_correlation_with_simlex999(simlex_df, glove_embeddings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Spearman Correlation Value with Wordsim-353 and glove.6B/glove.6B.50d.txt: 0.44779770696777327\n",
      "The Spearman Correlation Value with Wordsim-353 and glove.6B/glove.6B.100d.txt: 0.4783764811477666\n",
      "The Spearman Correlation Value with Wordsim-353 and glove.6B/glove.6B.200d.txt: 0.5160682763664167\n",
      "The Spearman Correlation Value with Wordsim-353 and glove.6B/glove.6B.300d.txt: 0.5433294686723303\n"
     ]
    }
   ],
   "source": [
    "glove_paths = [\n",
    "    'glove.6B/glove.6B.50d.txt',\n",
    "    'glove.6B/glove.6B.100d.txt',\n",
    "    'glove.6B/glove.6B.200d.txt',\n",
    "    'glove.6B/glove.6B.300d.txt'\n",
    "]\n",
    "\n",
    "for path in glove_paths:\n",
    "    glove_embeddings = load_glove_embeddings(path=path)\n",
    "    print(f'The Spearman Correlation Value with Wordsim-353 and {path}: {calculate_spearman_correlation_with_wordsim353(wordsim_df, glove_embeddings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blaze--experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
